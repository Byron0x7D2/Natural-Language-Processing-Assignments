{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzjy1bVMWD/ZCFxaLBdIqD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"2ba2a887fae748329979d5e224fb0c35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f413d6b8d4d340cc8501667f8b11a64e","IPY_MODEL_4fadc5cf86c348f5abf1c4179972c9fe","IPY_MODEL_ecaa3a3d04f14346b5ceb761ea4d548a"],"layout":"IPY_MODEL_9225aee318814cf994d4910e292b35c1"}},"f413d6b8d4d340cc8501667f8b11a64e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_64b016b495494420a9662cf0ff663ca1","placeholder":"​","style":"IPY_MODEL_c58effff38dd4b70b5e6b9168b8ef029","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"4fadc5cf86c348f5abf1c4179972c9fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae5470828a524fed96bcdbfa0198ecd9","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_607bbba086c54a7e8d439344b8e475fb","value":440473133}},"ecaa3a3d04f14346b5ceb761ea4d548a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9326e63c4f3e43e1a161dbe04c14a302","placeholder":"​","style":"IPY_MODEL_89b2cdb71ccc4c74928897a35d44e1ee","value":" 440M/440M [00:02&lt;00:00, 182MB/s]"}},"9225aee318814cf994d4910e292b35c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64b016b495494420a9662cf0ff663ca1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c58effff38dd4b70b5e6b9168b8ef029":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ae5470828a524fed96bcdbfa0198ecd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"607bbba086c54a7e8d439344b8e475fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9326e63c4f3e43e1a161dbe04c14a302":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89b2cdb71ccc4c74928897a35d44e1ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I8ATYicLfcWf","executionInfo":{"status":"ok","timestamp":1677072343526,"user_tz":-120,"elapsed":26751,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"460bf3a9-7bd9-4670-99ba-58978184ba05"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.8/dist-packages (0.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.5.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.1+cu116)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.26.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.8/dist-packages (2.0.0)\n","Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.8/dist-packages (from SPARQLWrapper) (6.2.0)\n","Requirement already satisfied: isodate in /usr/local/lib/python3.8/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (57.4.0)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from isodate->rdflib>=6.1.1->SPARQLWrapper) (1.15.0)\n"]}],"source":["import pandas as pd  #libraries loading\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import re\n","!pip install -q wordcloud\n","!pip install torchmetrics\n","import wordcloud\n","from torch.utils.data import Dataset\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger') \n","nltk.download('omw-1.4')\n","from torchtext.vocab import GloVe\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","from sklearn.model_selection import train_test_split\n","from matplotlib import pyplot as plt\n","from IPython.core.display import Path\n","from gensim.scripts.glove2word2vec import glove2word2vec\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets \n","from torchvision.transforms import ToTensor\n","import torch.nn.functional as F \n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader , RandomSampler\n","import random\n","import os\n","os.environ['PYTHONHASHSEED'] = str(69)\n","from sklearn.metrics import classification_report, RocCurveDisplay, roc_curve, auc\n","from sklearn.metrics import f1_score \n","from torchmetrics import ConfusionMatrix\n","!pip install transformers\n","from transformers import BertTokenizer, BertModel, BertTokenizerFast\n","!pip install SPARQLWrapper\n","from SPARQLWrapper import SPARQLWrapper, JSON\n","import pickle "]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","\n","def randreset(s):  #making sure each run will yield consistent results\n","\trandom.seed(s)\n","\tnp.random.seed(s)\n","\ttorch.manual_seed(s)\n","\ttorch.backends.cudnn.deterministic = True\n","\ttorch.backends.cudnn.benchmark = False\n","\ttorch.cuda.manual_seed_all(s)\n","\tos.environ['PYTHONHASHSEED'] = str(s)\n"," \n","\n","randreset(69)\n","bertmodel = 'bert-base-uncased'\n","tokenizer = BertTokenizerFast.from_pretrained(bertmodel)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRXRQD92nd3w","executionInfo":{"status":"ok","timestamp":1677072344342,"user_tz":-120,"elapsed":831,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"8f38d22a-9c28-4b21-c317-3cbd31996b42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["pathtrain = '/content/drive/My Drive/AI2/HW4/annotated_wd_data_train.txt'     #loading data from the files that we downloaded\n","pathval = '/content/drive/My Drive/AI2/HW4/annotated_wd_data_valid.txt'\n","pathtesting = '/content/drive/My Drive/AI2/HW4/annotated_wd_data_test.txt'\n","\n","dftrain = pd.read_csv(pathtrain,  sep='\\t', lineterminator='\\n', header=None)\n","dfval = pd.read_csv(pathval,  sep='\\t', lineterminator='\\n', header=None)\n","dftesting = pd.read_csv(pathtesting,  sep='\\t', lineterminator='\\n', header=None)\n","\n","\n","max = dftesting[3].map(lambda r: len(r.split())).max()\n","print(max)\n","max = dftrain[3].map(lambda r: len(r.split())).max()\n","print(max)\n","max = dfval[3].map(lambda r: len(r.split())).max()\n","print(max)\n","\n","# dfval[1]\n","# relations = set()      #keeping track of all the entities and relations of the dataset that we are interested in\n","# relations.update(dftesting[1])\n","# relations.update(dfval[1])\n","# relations.update(dftrain[1])\n","# relations = list(relations)\n","# print(len(relations))   #doing this so we have a constant lexic for the other files\n","relations = ['R376', 'R131', 'P115', 'R176', 'P800', 'P113', 'P404', 'P509', 'P1408', 'P144', 'P59', 'R17', 'R119', 'P17', 'P264', 'R676', 'P272', 'R105', 'P105', 'P21', 'R162', 'P397', 'P1040', 'P31', 'P140', 'R279', 'P189', 'P112', 'P138', 'R1040', 'R361', 'P407', 'R179', 'R272', 'P131', 'P170', 'R58', 'P119', 'R136', 'R86', 'P376', 'R421', 'R106', 'P50', 'P1142', 'R40', 'R607', 'R112', 'R175', 'R1431', 'P641', 'P1431', 'P738', 'P607', 'P172', 'P196', 'R264', 'P413', 'P495', 'R178', 'P175', 'P57', 'R177', 'P150', 'R123', 'R170', 'R57', 'P676', 'R138', 'P156', 'R161', 'P20', 'P58', 'P19', 'P344', 'R115', 'P289', 'R149', 'R144', 'P279', 'P27', 'P421', 'R172', 'R21', 'R413', 'R641', 'R59', 'P398', 'R19', 'P403', 'P710', 'P179', 'P155', 'R344', 'P276', 'R50', 'P826', 'P53', 'P1308', 'R171', 'R404', 'P40', 'R289', 'P61', 'P136', 'P86', 'R287', 'R31', 'P123', 'P364', 'P1303', 'P287', 'P178', 'P177', 'P171', 'R1142', 'P84', 'P176', 'R509', 'P361', 'R276', 'P81', 'P737', 'P162', 'P65', 'P106', 'R1303', 'P149', 'P1029']\n","\n","\n","entities = set()\n","entities.update(dftrain[0])\n","entities.update(dfval[0])\n","entities.update(dftesting[0])\n","print(len(entities))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PL0fFCohf5OS","executionInfo":{"status":"ok","timestamp":1677073458783,"user_tz":-120,"elapsed":687,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"11fed219-b930-4c39-fe90-1f386901207b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["24\n","33\n","20\n","40284\n"]}]},{"cell_type":"markdown","source":["# Relation Prediction"],"metadata":{"id":"TOgqMb-kdxQn"}},{"cell_type":"code","source":["class CustomDataset(Dataset): #same as in the IMDB reviews part\n","  def __init__(self, X, Y, tokenizer, maxlen):\n","    self.X = []\n","    self.Y = []\n","    self.tokenizer = tokenizer\n","    self.maxlen = maxlen\n","\n","    for (x,y) in zip(X,Y):\n","      self.X.append(self.tokenizer.encode_plus(x, add_special_tokens=True, max_length=self.maxlen, padding='max_length', truncation=True, return_tensors='pt' , return_token_type_ids=False, return_attention_mask=True).to(device))\n","      self.Y.append(torch.tensor(y, dtype=torch.long, device=device))\n","\n","  def __len__(self):\n","    return len(self.Y)\n","\n","  def __getitem__(self, index):\n","    return (self.X[index]['input_ids'], self.X[index]['attention_mask'], self.Y[index])\n","  "],"metadata":{"id":"fzFJvGFtC-Ar"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtrain = list(dftrain[3].values)   \n","Ytrain = list(dftrain[1].apply(lambda r: relations.index(r)))  #the target is the classification of relation out of all the possible relations in the dataset\n","traind = CustomDataset(Xtrain, Ytrain, tokenizer, 33)\n","\n","Xval = list(dfval[3].values)\n","Yval = list(dfval[1].apply(lambda r: relations.index(r)))\n","vald = CustomDataset(Xval, Yval, tokenizer, 33)\n","\n","Xtesting = list(dftesting[3].values)\n","Ytesting =  list(dftesting[1].apply(lambda r: relations.index(r)))\n","testingd = CustomDataset(Xtesting, Ytesting, tokenizer, 33)\n","\n","\n","train_dataloader = DataLoader(traind , batch_size=40, sampler=RandomSampler(traind))\n","val_dataloader= DataLoader(vald , batch_size=40,sampler=RandomSampler(vald))\n","testing_dataloader = DataLoader(testingd , batch_size=40,sampler=RandomSampler(testingd))"],"metadata":{"id":"aT6MTKW4sO8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTmodel(nn.Module):\n","  def __init__(self, dr, H, activation):\n","    super(BERTmodel, self).__init__()\n","    self.bertlayer = BertModel.from_pretrained(bertmodel).to(device)\n","    self.linear = nn.Sequential(\n","          nn.Linear(768, H, device=device),   #768 is the output of the bert model\n","          activation(),\n","\t\t      nn.Dropout(dr),\n","          nn.Linear(H,129, device=device),  #output is the distribution for the 129 possible relations, this is the only change from the IMDB part since both are basically classifications\n","        )\n","\n","  def forward(self, ids, mask):\n","    out = self.bertlayer(ids, mask)\n","    return self.linear(out[0][:, 0, :])\n","\n","  def predict(self, dataloader):\n","    pred = torch.tensor([], device='cpu')\n","    true = torch.tensor([], dtype=torch.int, device='cpu')  \n","    with torch.no_grad():\n","      self.eval()\n","      for (inputid, attentionmask, labels) in dataloader:\n","        _, y_pred = torch.max(self(inputid.squeeze(), attentionmask.squeeze()), 1)\n","        pred = torch.cat((pred, y_pred.cpu()))\n","        true = torch.cat((true, labels.squeeze().cpu()))\n","      return pred, true\n","\n","def train(model, traindataloader, valdataloader, optimiser, learningrate, maxepochs):\n","  lastf1 = None\n","  opt = getattr(torch.optim, optimiser)(model.parameters(), lr=learningrate) \n","  lossfunction = nn.CrossEntropyLoss()\n","  for epoch in range(maxepochs):\n","    model.train()\n","    t_losses = []\n","    for (inputid, attentionmask, labels) in traindataloader:\n","      opt.zero_grad()\n","      ypred = model(inputid.squeeze(), attentionmask.squeeze())\n","      loss = lossfunction(ypred, labels.squeeze())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","      opt.step()\n","\n","    pred, true = model.predict(valdataloader)\n","    f1 = f1_score(true, pred, average='weighted')\n","    print(f\"for epoch: {epoch} we got val f1 score = {f1}\")\n","\n","    if lastf1 is None:    #stopping early if validation f1 does not improve\n","      lastf1 = f1\n","    else: \n","      if lastf1 >= f1:\n","        break\n","      else:\n","        lastf1 = f1\n","  \n","  return model\n","\n"],"metadata":{"id":"kBdMc1eOucIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTmodel(0.1, 54, nn.CELU)  #same as the IMDB part since it works well\n","model = train(model, train_dataloader, val_dataloader, \"AdamW\", 2.2e-05, 4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191,"referenced_widgets":["2ba2a887fae748329979d5e224fb0c35","f413d6b8d4d340cc8501667f8b11a64e","4fadc5cf86c348f5abf1c4179972c9fe","ecaa3a3d04f14346b5ceb761ea4d548a","9225aee318814cf994d4910e292b35c1","64b016b495494420a9662cf0ff663ca1","c58effff38dd4b70b5e6b9168b8ef029","ae5470828a524fed96bcdbfa0198ecd9","607bbba086c54a7e8d439344b8e475fb","9326e63c4f3e43e1a161dbe04c14a302","89b2cdb71ccc4c74928897a35d44e1ee"]},"id":"w-rVS6iyutrz","executionInfo":{"status":"ok","timestamp":1677073234622,"user_tz":-120,"elapsed":856453,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"a0999a1a-f232-4df8-ac19-c2a65ef50aa0"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ba2a887fae748329979d5e224fb0c35"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 0 we got val f1 score = 0.9414160051247691\n","for epoch: 1 we got val f1 score = 0.9609732365431753\n","for epoch: 2 we got val f1 score = 0.963538419023227\n","for epoch: 3 we got val f1 score = 0.9603346150752379\n"]}]},{"cell_type":"code","source":["pred, true = model.predict(testing_dataloader)\n","print(f\"test f1 score is {f1_score(true, pred, average='weighted')}\")\n","print(classification_report(true, pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lPD-ojZ4zEjK","executionInfo":{"status":"ok","timestamp":1677073501356,"user_tz":-120,"elapsed":17891,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"eed429fe-1fa6-491c-f83b-74ab4888dec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["test f1 score is 0.958067331213427\n","              precision    recall  f1-score   support\n","\n","           1       0.96      0.93      0.95        29\n","           2       1.00      1.00      1.00         7\n","           4       1.00      1.00      1.00         5\n","           5       1.00      1.00      1.00         7\n","           6       1.00      0.95      0.97        57\n","           7       0.98      0.99      0.99       116\n","           8       1.00      0.92      0.96        13\n","           9       0.71      1.00      0.83         5\n","          10       0.43      1.00      0.60         3\n","          11       0.91      0.95      0.93        21\n","          12       1.00      1.00      1.00         8\n","          13       0.96      0.79      0.87        67\n","          14       0.97      0.96      0.96       157\n","          15       1.00      1.00      1.00        17\n","          16       0.90      0.92      0.91        39\n","          17       1.00      1.00      1.00         3\n","          18       0.83      0.91      0.87        11\n","          19       1.00      0.99      1.00       473\n","          20       0.76      0.98      0.86        52\n","          21       1.00      1.00      1.00        50\n","          22       1.00      0.94      0.97        18\n","          23       0.95      0.97      0.96        64\n","          24       1.00      0.99      0.99        99\n","          25       0.00      0.00      0.00         2\n","          27       0.88      0.97      0.92        29\n","          28       1.00      0.95      0.97        40\n","          29       1.00      1.00      1.00         7\n","          30       0.00      0.00      0.00         1\n","          31       0.93      0.57      0.70        23\n","          32       0.77      0.83      0.80        12\n","          33       1.00      0.89      0.94        28\n","          34       0.77      0.90      0.83        41\n","          35       0.79      0.95      0.86        63\n","          36       0.91      0.92      0.91        73\n","          37       0.86      1.00      0.92        12\n","          38       0.99      0.98      0.98       863\n","          39       0.94      0.91      0.92        32\n","          40       0.00      0.00      0.00         1\n","          41       1.00      0.97      0.99        38\n","          42       0.92      0.87      0.89       117\n","          43       0.80      0.73      0.76        51\n","          44       0.85      1.00      0.92        11\n","          45       1.00      0.98      0.99        42\n","          46       0.86      0.82      0.84        22\n","          47       1.00      0.96      0.98        28\n","          48       0.85      0.94      0.89        85\n","          49       1.00      0.42      0.59        26\n","          50       0.80      1.00      0.89         4\n","          51       1.00      0.76      0.87        17\n","          52       0.89      0.86      0.88        59\n","          53       1.00      1.00      1.00        49\n","          54       0.99      0.97      0.98        99\n","          55       1.00      0.92      0.96        25\n","          56       0.99      0.99      0.99        82\n","          57       1.00      0.99      0.99       329\n","          58       0.98      0.95      0.96       330\n","          59       1.00      0.70      0.82        20\n","          60       0.90      0.92      0.91       164\n","          61       0.98      0.99      0.99       105\n","          62       0.80      1.00      0.89         4\n","          63       0.67      0.67      0.67         3\n","          64       0.76      1.00      0.86        22\n","          65       0.91      0.94      0.92        32\n","          66       0.90      0.95      0.92        56\n","          67       0.82      0.88      0.85        26\n","          68       0.94      0.94      0.94        32\n","          69       0.75      0.75      0.75         4\n","          71       0.99      0.97      0.98       345\n","          72       0.82      0.90      0.86        90\n","          73       0.97      0.95      0.96       597\n","          74       1.00      0.87      0.93        15\n","          75       1.00      1.00      1.00         4\n","          76       1.00      0.50      0.67         2\n","          77       1.00      0.93      0.97        15\n","          78       1.00      0.75      0.86         8\n","          79       0.00      0.00      0.00         2\n","          80       0.92      0.98      0.95       571\n","          81       0.98      1.00      0.99       116\n","          82       0.94      0.88      0.91        56\n","          83       1.00      1.00      1.00         9\n","          84       0.97      0.99      0.98       108\n","          85       1.00      1.00      1.00         3\n","          86       0.00      0.00      0.00         5\n","          87       0.94      1.00      0.97        16\n","          88       0.98      0.99      0.99       351\n","          89       1.00      1.00      1.00        11\n","          90       0.93      0.83      0.87        63\n","          91       0.78      1.00      0.88        14\n","          92       1.00      0.50      0.67         4\n","          93       0.85      0.69      0.76        16\n","          94       0.80      0.95      0.87        39\n","          95       1.00      0.68      0.81        28\n","          96       1.00      0.50      0.67         2\n","          97       1.00      1.00      1.00        14\n","          99       1.00      0.80      0.89         5\n","         100       0.96      1.00      0.98        27\n","         101       0.98      0.98      0.98        41\n","         102       1.00      1.00      1.00         1\n","         103       1.00      1.00      1.00        10\n","         104       0.99      0.99      0.99      1769\n","         105       0.93      0.88      0.91       109\n","         106       0.75      1.00      0.86         3\n","         107       1.00      0.40      0.57         5\n","         108       0.98      0.91      0.95        47\n","         109       0.95      0.98      0.97       375\n","         110       1.00      0.99      1.00       101\n","         111       1.00      1.00      1.00         3\n","         112       0.93      0.93      0.93        28\n","         113       1.00      1.00      1.00         8\n","         114       0.73      0.80      0.76        10\n","         115       1.00      0.83      0.91         6\n","         116       1.00      1.00      1.00         8\n","         117       0.00      0.00      0.00         1\n","         118       0.98      1.00      0.99        62\n","         119       0.00      0.00      0.00         1\n","         120       1.00      1.00      1.00        32\n","         121       1.00      1.00      1.00         2\n","         122       0.84      0.80      0.82        40\n","         123       0.90      0.98      0.94        53\n","         124       0.86      1.00      0.92        12\n","         125       0.94      0.99      0.97       181\n","         126       1.00      0.98      0.99        52\n","         127       0.94      0.97      0.95        32\n","         128       1.00      1.00      1.00         2\n","\n","    accuracy                           0.96      9960\n","   macro avg       0.88      0.86      0.86      9960\n","weighted avg       0.96      0.96      0.96      9960\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/My Drive/AI2/HW4/relationpredictor.pt')"],"metadata":{"id":"RLnQM4ZajyLa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Span Prediction\n"],"metadata":{"id":"os3YepTRdq-1"}},{"cell_type":"code","source":["# sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")     #doing the required queries, saving the results since there are limits\n","# import time\n","# index = 0\n","# # stops = [item for item in range(1000, 40000, 1000)]\n","# index = 39965\n","# entities = list(entities)\n","# for ent in entities[39965:]:\n","#   sparql.setQuery(\"\"\"\n","# PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#> \n","# PREFIX wd: <http://www.wikidata.org/entity/> \n","# SELECT  *\n","# WHERE {\n","#         wd:%s rdfs:label ?label .\n","#         FILTER (langMatches( lang(?label), \"EN\" ) )\n","#       }\n","# \"\"\" %(ent))\n","  \n","#   sparql.setReturnFormat(JSON)\n","#   results = sparql.query().convert()\n","\n","#   results_df = pd.json_normalize(results['results']['bindings'])\n","#   if set([\"label.value\"]).issubset(results_df):\n","#     # print(results_df[\"label.value\"][0])\n","#     # entitylabels[ent] = results_df[\"label.value\"][0]\n","#     f = open(\"/content/drive/My Drive/AI2/HW4/dump.txt\", \"a\")\n","#     f.write(f'{ent}  {results_df[\"label.value\"][0]} \\n')\n","#     f.close()\n","#   print(index)\n","\n","#   # if index in stops:\n","#   #   time.sleep(305)\n","#   index+=1\n","\n"],"metadata":{"id":"_8JSPu8K9dyd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# f = open(\"/content/drive/My Drive/AI2/HW4/dump.txt\", \"r\")\n","# entitylabels = {}\n","\n","# for i in range(39925):\t\n","#   line = f.readline()\n","#   tok = line.split()\n","#   entitylabels[tok[0]] = tok[1]\n","\n","# with open('/content/drive/My Drive/AI2/HW4/saved_dictionary.pkl', 'wb') as f:\n","#   pickle.dump(entitylabels, f)"],"metadata":{"id":"gcI_lpWx4Peq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open('/content/drive/My Drive/AI2/HW4/saved_dictionary.pkl', 'rb') as f:   #loading the dictionary with the entities and their label\n","  entitylabels = pickle.load(f)\n","\n","# print(entitylabels)"],"metadata":{"id":"FaDwdR6yG8aQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def createspantarget(entitylabels, text, labels):  #output 33 size vectors with 0-1\n","  spans = []\n","  for (x,label) in zip(text, labels):\n","    tokens = x.split()\n","\n","    if label not in entitylabels:    #if we cant find the label for whatever reason we pass an all zeroes vector\n","      spans.append([0] * 33)\n","      continue\n","\n","    target = entitylabels[label]\n","    targettokens = target.split()\n","    if(x.rfind(target) == -1):  #if the substring is not in the question we wont be able to find it so we move on with a zeor vector\n","      spans.append([0] * 33)\n","    else:\n","      l = [0] * 33\n","      for i,w in enumerate(tokens):\n","        if w in targettokens:    #whenever we find a part of teh question on the substring, we put a 1 in that place\n","          l[i] = 1;\n","      spans.append(l)\n","\n","  return spans"],"metadata":{"id":"9HX-_7MpT2Ha"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SpanCustomDataset(Dataset):\n","  def __init__(self, X, Y, tokenizer, maxlen):\n","    self.X = []\n","    self.Y = []\n","    self.tokenizer = tokenizer\n","    self.maxlen = maxlen\n","\n","    for (x,y) in zip(X,Y):\n","      if 1 in y:                #only if we managed to find the span we keep the question on the dataset\n","        self.X.append(self.tokenizer.encode_plus(x, add_special_tokens=True, max_length=self.maxlen, padding='max_length', truncation=True, return_tensors='pt' , return_token_type_ids=False, return_attention_mask=True).to(device))\n","        self.Y.append(torch.Tensor(y).to(device))\n","\n","\n","  def __len__(self):\n","    return len(self.Y)\n","\n","  def __getitem__(self, index):\n","    return (self.X[index]['input_ids'], self.X[index]['attention_mask'], self.Y[index])"],"metadata":{"id":"fm3mgs3Uczyy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Xtrain = list(dftrain[3].values)\n","Ytrain = createspantarget(entitylabels, Xtrain, dftrain[0])\n","traind = SpanCustomDataset(Xtrain, Ytrain, tokenizer, 33)\n","\n","Xval = list(dfval[3].values)\n","Yval = createspantarget(entitylabels, Xval, dfval[0])\n","vald = SpanCustomDataset(Xval, Yval, tokenizer, 33)\n","\n","Xtesting = list(dftesting[3].values)\n","Ytesting = createspantarget(entitylabels, Xtesting, dftesting[0])\n","testingd = SpanCustomDataset(Xtesting, Ytesting, tokenizer, 33)\n","\n","train_dataloader = DataLoader(traind , batch_size=40, sampler=RandomSampler(traind))\n","val_dataloader= DataLoader(vald , batch_size=40,sampler=RandomSampler(vald))\n","testing_dataloader = DataLoader(testingd , batch_size=40,sampler=RandomSampler(testingd))"],"metadata":{"id":"cOJQFDEFZJ8f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BERTmodelforSpan(nn.Module):\n","  def __init__(self, dr, H, activation):\n","    super(BERTmodelforSpan, self).__init__()\n","    self.bertlayer = BertModel.from_pretrained(bertmodel).to(device)\n","    self.linear = nn.Sequential(\n","          nn.Linear(768, H, device=device),   #768 is the output of the bert model\n","          activation(),\n","\t\t      nn.Dropout(dr),\n","          nn.Linear(H,33, device=device),   #output is 33 for the 33 places on the vector for the span\n","        )\n","\n","  def forward(self, ids, mask):\n","    out = self.bertlayer(ids, mask)\n","    return torch.sigmoid(self.linear(out[0][:, 0, :]))\n","\n","  def predict(self, dataloader):\n","    pred = torch.tensor([], device='cpu')\n","    true = torch.tensor([], dtype=torch.int, device='cpu')  #using tensors so I can control that they are in the cpu in order for sklearn's functions to not complain\n","    with torch.no_grad():\n","      self.eval()\n","      for (inputid, attentionmask, labels) in dataloader:\n","        y_pred = torch.round(self(inputid.squeeze(), attentionmask.squeeze()))             \n","        pred = torch.cat((pred, y_pred.cpu()))\n","        true = torch.cat((true, labels.squeeze().cpu()))\n","      return pred, true\n","\n","def train(model, traindataloader, valdataloader, optimiser, learningrate, maxepochs):\n","  lastf1 = None\n","  opt = getattr(torch.optim, optimiser)(model.parameters(), lr=learningrate) \n","  lossfunction = nn.BCELoss()\n","  for epoch in range(maxepochs):\n","    model.train()\n","    t_losses = []\n","    for (inputid, attentionmask, labels) in traindataloader:\n","      opt.zero_grad()\n","      ypred = model(inputid.squeeze(), attentionmask.squeeze())\n","      loss = lossfunction(ypred, labels.squeeze())\n","      loss.backward()\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","      opt.step()\n","\n","    pred, true = model.predict(valdataloader)\n","    f1 = f1_score(true, pred, average='weighted')\n","    print(f\"for epoch: {epoch} we got val f1 score = {f1}\")\n","\n","    if lastf1 is None:    #stopping early if validation f1 does not improve\n","      lastf1 = f1\n","    else: \n","      if lastf1 >= f1:\n","        break\n","      else:\n","        lastf1 = f1\n","  \n","  return model"],"metadata":{"id":"TrWbeztIeWc1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = BERTmodelforSpan(0.1, 54, nn.CELU)\n","model = train(model, train_dataloader, val_dataloader, \"AdamW\", 2.2e-05, 15)\n","pred, true = model.predict(val_dataloader)\n","print(f\"test f1 score is {f1_score(true, pred, average='weighted')}\")\n","print(classification_report(true, pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wIYYm-tojlW","executionInfo":{"status":"ok","timestamp":1677074297103,"user_tz":-120,"elapsed":312240,"user":{"displayName":"Byronas A","userId":"05026113568586961416"}},"outputId":"176df165-9ffd-4215-fc5c-584e16685272"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 0 we got val f1 score = 0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 1 we got val f1 score = 0.054613288824525666\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 2 we got val f1 score = 0.09377441095790871\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 3 we got val f1 score = 0.3064274107111987\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 4 we got val f1 score = 0.7210217445412781\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 5 we got val f1 score = 0.7534009753433418\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 6 we got val f1 score = 0.7866995086214587\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 7 we got val f1 score = 0.8129003603007016\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 8 we got val f1 score = 0.861943957120547\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 9 we got val f1 score = 0.8841512207243134\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"]},{"output_type":"stream","name":"stdout","text":["for epoch: 10 we got val f1 score = 0.8697744865153243\n","test f1 score is 0.8697744865153243\n","              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         7\n","           1       1.00      0.78      0.88        27\n","           2       0.99      0.97      0.98       127\n","           3       0.98      0.99      0.98       140\n","           4       0.96      0.91      0.93        78\n","           5       0.88      0.87      0.87        92\n","           6       0.92      0.89      0.90        89\n","           7       0.86      0.56      0.68        32\n","           8       0.00      0.00      0.00        15\n","           9       0.00      0.00      0.00         7\n","          10       0.00      0.00      0.00         3\n","          11       0.00      0.00      0.00         2\n","          12       0.00      0.00      0.00         2\n","          13       0.00      0.00      0.00         1\n","          14       0.00      0.00      0.00         0\n","          15       0.00      0.00      0.00         0\n","          16       0.00      0.00      0.00         0\n","          17       0.00      0.00      0.00         0\n","          18       0.00      0.00      0.00         0\n","          19       0.00      0.00      0.00         0\n","          20       0.00      0.00      0.00         0\n","          21       0.00      0.00      0.00         0\n","          22       0.00      0.00      0.00         0\n","          23       0.00      0.00      0.00         0\n","          24       0.00      0.00      0.00         0\n","          25       0.00      0.00      0.00         0\n","          26       0.00      0.00      0.00         0\n","          27       0.00      0.00      0.00         0\n","          28       0.00      0.00      0.00         0\n","          29       0.00      0.00      0.00         0\n","          30       0.00      0.00      0.00         0\n","          31       0.00      0.00      0.00         0\n","          32       0.00      0.00      0.00         0\n","\n","   micro avg       0.95      0.85      0.90       622\n","   macro avg       0.20      0.18      0.19       622\n","weighted avg       0.89      0.85      0.87       622\n"," samples avg       0.85      0.85      0.85       622\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), '/content/drive/My Drive/AI2/HW4/spandetector.pt')"],"metadata":{"id":"wStQ9REDj4eY"},"execution_count":null,"outputs":[]}]}